{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Operations\n",
    "First, let's discuss implementation of the four supported operations of our shift-reduce parser.  As discussed in [part 1]({{ base.url }}/2016/08/umich_nlp_depparse_intro/), the four supported operations are `left_arc()`, `right_arc()`, `shift()`, and `reduce()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left Arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def left_arc(conf, relation):\n",
    "    if not conf.buffer:\n",
    "        return -1\n",
    "\n",
    "    if conf.stack[-1] == 0:\n",
    "        return -1\n",
    "\n",
    "    for arc in conf.arcs:\n",
    "        if conf.stack[-1] == arc[Transition.ARC_CHILD]:\n",
    "            return -1\n",
    "\n",
    "    b = conf.buffer[0]\n",
    "    s = conf.stack.pop(-1)\n",
    "    # Add the arc (b, L, s)\n",
    "    conf.arcs.append((b, relation, s))\n",
    "    pass\n",
    "# END left_arc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Right Arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def right_arc(conf, relation):\n",
    "    if not conf.buffer or not conf.stack:\n",
    "        return -1\n",
    "\n",
    "    s = conf.stack[-1]\n",
    "    b = conf.buffer.pop(0)\n",
    "\n",
    "    conf.stack.append(b)\n",
    "    conf.arcs.append((s, relation, b))\n",
    "    pass\n",
    "# END right_arc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce(conf):\n",
    "    if not conf.stack:\n",
    "        return -1\n",
    "\n",
    "    for arc in conf.arcs:\n",
    "        if conf.stack[-1] == arc[Transition.ARC_CHILD]:\n",
    "            s = conf.stack.pop(-1)\n",
    "            return\n",
    "    return -1\n",
    "# END reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift(conf):\n",
    "    if not conf.buffer or not conf.stack:\n",
    "        return -1\n",
    "\n",
    "    b = conf.buffer.pop(0)\n",
    "    conf.stack.append(b)\n",
    "    pass\n",
    "# END shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As might be expected, the implementation of the functions is straightforward.  No explanation of the logic is given, but I think the reader should be able to interpret fairly easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of Features\n",
    "The four functions of the shift-reduce parser above define the actions that are available for a given configuration, but we need a brain to tell the program when to apply which function.  As discussed in part 1, we could try to create some hard-coded rules based on various configurations.  However, with this approach, the program would be inflexible and likely perform poorly.  Instead, we can use supervised machine learning to have the program learn for itself how to apply the shift-reduce functions.\n",
    "\n",
    "As with any supervised learning problem, we need two things - a set of golden data to train the machine, and a method of extracting useful features from that golden data.  For this problem, we were provided golden data in the form of CONLL datasets for english, danish, and swedish.  This dataset essentially provides a series of configurations accompanied by the correct operation and label (if applicable).  Our assignment was to extract \"features\" from each configuration - properties about the configuration which provide positive predictive value for the operation to be used.\n",
    "\n",
    "I made multiple iterations to converge to a solution; these are outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Structures\n",
    "Before getting into the iterations on the feature extractor, let's first define the data structure that is used the the extractor.  As discussed previously, there are three components of a parser configuration.  $B$ is the buffer, remaining words to be parsed.  $\\Sigma$ is the stack, holding words that have been processed via the `right_arc()` or `shift()` operations.  $A$ is the set of arcs that have been added to the dependency graph.\n",
    "\n",
    "Note that $B$ and $\\Sigma$ both contain words, which may have a variety of properties.  Therefore it may make sense to index those words and store them in a separate data structure.  Let's call that $T$, a list of dictionaries storing the properties of each word.\n",
    "\n",
    "Let's take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named providedcode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cf75bd497e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovidedcode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_english_train_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msmalldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named providedcode"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from code.providedcode import dataset\n",
    "data = dataset.get_english_train_corpus.parsed_sents()\n",
    "smalldata = random.sample(data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration #1 - Coarse POS\n",
    "The first iteration of the feature extractor only made use of the coarse-grained part of speech for the top word in the stack and the buffer.  For example, $S=['company',...]$ and $\\Sigma=['was',...]$ might identify two features.  \"Company\" would be a \"NOUN\" and \"was\" would be a \"VERB\".\n",
    "\n",
    "In code, this might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5b6079f13105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"STK_0_TAG_{0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stack' is not defined"
     ]
    }
   ],
   "source": [
    "s = stack[-1]\n",
    "tok = tokens[s]\n",
    "result = \"STK_0_TAG_{0}\".format(tok['tag'].upper())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
